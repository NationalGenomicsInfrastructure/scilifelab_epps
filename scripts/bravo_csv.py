#!/usr/bin/env python

from __future__ import division
import logging
import os
import sys
import re
import pandas as pd
from argparse import ArgumentParser
from genologics.lims import Lims
from genologics.config import BASEURI, USERNAME, PASSWORD
from scilifelab_epps.epp import attach_file
from genologics.entities import Process
from numpy import minimum, where
from datetime import date

DESC = """EPP used to create csv files for the bravo robot"""

MAX_WARNING_VOLUME = 150.0
MIN_WARNING_VOLUME = 2.0

# Three values are minimum required conc for setup workset, maximum conc for dilution and minimum volume for dilution
Dilution_preset = {
    "Smarter pico": [1.25, 375.0, 10.0]
}

# Pre-compile regexes in global scope:
IDX_PAT = re.compile("([ATCG]{4,})-?([ATCG]*)")
TENX_PAT = re.compile("SI-GA-[A-H][1-9][0-2]?")

def obtain_previous_volumes(currentStep, lims):
    samples_volumes = {}
    re_well = re.compile("([A-H]):?0?([0-9]{1,2})")
    previous_steps = set()
    for input_artifact in currentStep.all_inputs():
        previous_steps.add(input_artifact.parent_process)
    for pp in previous_steps:
        for output in pp.all_outputs():
            if output.name == "EPP Generated Bravo CSV File for Normalization":
                try:
                    fid = output.files[0].id
                except:
                    raise RuntimeError("Cannot access the normalisation CSV file to read the volumes.")
                else:
                    file_contents = lims.get_file_contents(id=fid)
                    if isinstance(file_contents, bytes):
                        file_contents = file_contents.decode('utf-8')
                    genologics_format = False
                    well_idx = 4
                    plate_idx = 3
                    source_vol_idx = 2
                    buffer_vol_idx = 5
                    for line in file_contents.split('\n'):
                        # Skip some lines:
                        if not line.rstrip():
                            continue
                        elif "Date of file generation:" in line:
                            continue
                        elif "Generated by:" in line:
                            continue
                        elif "Sample Name" in line:
                            # This is Genologics format and the header line
                            # so change column indices:
                            genologics_format = True
                            elements = line.split(',')
                            for idx, el in enumerate(elements):
                                if el == "Source Volume (uL)":
                                    source_vol_idx = idx
                                elif el == "Volume of Dilution Buffer (uL)":
                                    buffer_vol_idx = idx
                                elif el == "Destination Well":
                                    well_idx = idx
                                elif el == "Destination Plate":
                                    plate_idx = idx
                                elif el == "Sample Name":
                                    name_idx = idx
                        else:
                            elements = line.split(',')
                            well = elements[well_idx]
                            matches = re_well.search(well)
                            if matches:
                                well = ":".join(x for x in matches.groups())
                            plate = elements[plate_idx]
                            srcvol = elements[source_vol_idx]
                            bufvol = elements[buffer_vol_idx]
                            # Remove any quotes:
                            (plate, srcvol, bufvol) = [s.replace('"', '') for s in (plate, srcvol, bufvol)]
                            srcvol = float(srcvol)
                            bufvol = float(bufvol)
                            totvol = bufvol
                            # For Genologics format compability:
                            if genologics_format:
                                totvol += srcvol
                                name = elements[name_idx].replace('"', '')
                                samples_volumes[name] = totvol
                            else:
                                samples_volumes.setdefault(plate, {})[well] = totvol
    return samples_volumes


def make_datastructure(currentStep, lims, log):
    data = []
    samples_volumes = {}
    try:
        samples_volumes = obtain_previous_volumes(currentStep, lims)
    except:
        log.append("Unable to find previous volumes")
        sys.stderr.write("Samples volumes cannot be found. Check the file from the previous step")
        sys.exit(2)

    for inp, out in currentStep.input_output_maps:
        if out['output-type'] == 'Analyte':
            obj = {}
            obj['name'] = inp['uri'].samples[0].name
            obj['id'] = inp['uri'].id
            if "Normalized conc. (nM)" in inp['uri'].udf:
                obj['conc'] = inp['uri'].udf['Normalized conc. (nM)']
            else:
                obj['conc'] = inp['uri'].udf['Concentration']
            obj['pool_id'] = out['uri'].id
            # obj['pool_conc']=out['uri'].udf['Normalized conc. (nM)']
            obj['src_fc'] = inp['uri'].location[0].name
            obj['src_fc_id'] = inp['uri'].location[0].id
            obj['src_well'] = inp['uri'].location[1]
            obj['dst_fc'] = out['uri'].location[0].id
            obj['dst_well'] = out['uri'].location[1]
            # For Genologics format compability:
            if obj['name'] in samples_volumes:
                obj['vol'] = samples_volumes[obj['name']]
            # Try to match container ID first then name:
            elif obj['src_fc_id'] in samples_volumes:
                obj['vol'] = samples_volumes[obj['src_fc_id']][obj['src_well']]
            elif "Volume (ul)" in inp['uri'].udf:
                obj['vol']=inp['uri'].udf["Volume (ul)"]
            else:
                try:
                    obj['vol'] = samples_volumes[obj['src_fc']][obj['src_well']]
                except KeyError:
                    obj['vol'] = None
                    log.append("Unable to find previous volume for {}".format(obj["name"]))

            data.append(obj)

    return data

""" LAZY WAY
 Just divide the total with the number of samples, it is implied that the final
 conc and the conc of every input is the same:
"""


def lazy_volumes(samples, final_vol):
    return [final_vol / len(samples) for s in samples]

""" OTHER WAY
 Iteratively reduce the smallest input volume until we are close to the desired
 total volume. This works when the inputs have different concentrations, the
 final pool concentration will then end up somewhere between the conc of the
 highest and the lowest input concentration:
"""


def optimize_volumes(samples, final_vol, limit_vol=2):
    # Create a list we can sort to get the min/max values:
    l = [(s["conc"] * s["vol"], s["conc"], s["vol"]) for s in samples]
    # Find the min/max values by sorting on the different values:
    min_conc = sorted(l, key=lambda x: x[1])[0][1]  # unused
    max_conc = sorted(l, key=lambda x: x[1])[-1][1]
    min_vol = sorted(l, key=lambda x: x[2])[0][2]
    # The volume of the input with lowest amount:
    min_amount = sorted(l)[0][2]

    def _minimize_vol(vol, final_vol=final_vol, limit_vol=limit_vol, reduce=0.9):
        try_vol = reduce * vol
        # The lowest volume to take would then be (sample(s) w highest conc):
        low_vol = min(try_vol * max_conc / s["conc"] for s in samples)
        # Total pool volume if we were to take this amount of all samples:
        tot_vol = sum(try_vol * max_conc / s["conc"] for s in samples)
        # We don't want to pipette less than limit_vol
        # while keeping total volume above final_vol:
        if low_vol >= limit_vol and tot_vol >= final_vol and try_vol >= limit_vol:
            return _minimize_vol(try_vol)
        else:
            # We can't improve anymore within the given limits...
            return vol

    # Start from whichever is the smallest volume:
    use_vol = _minimize_vol(min(min_amount, min_vol))
    # Calculate the volume to take of each input:
    return [(use_vol * max_conc / s["conc"]) for s in samples]


def compute_transfer_volume(currentStep, lims, log):
    data = make_datastructure(currentStep, lims, log)
    returndata = []
    for pool in currentStep.all_outputs():
        if pool.type == 'Analyte':
            valid_inputs = [x for x in data if x['pool_id'] == pool.id]
            # Set the output conc of the pool and also get the "desired" pool
            # volume, which is which?
            final_vol = float(pool.udf["Final Volume (uL)"])
            conc = valid_inputs[0]["conc"]
            # If all inputs are of the same conc use the trivial algorithm,
            # else try to optimize:
            if all(s["conc"] == conc for s in valid_inputs):
                vols = lazy_volumes(valid_inputs, final_vol)
                pool.udf['Normalized conc. (nM)'] = conc
            else:
                vols = optimize_volumes(valid_inputs, final_vol, MIN_WARNING_VOLUME)
                # Calculate and add the theoretical pool conc:
                z = list(zip([s["conc"] for s in valid_inputs], vols))
                v = (sum(x[0] * x[1] for x in z) / sum(vols))
                pool.udf['Normalized conc. (nM)'] = v
            pool.put()
            for s, vol in zip(valid_inputs, vols):
                s['vol_to_take'] = vol
                returndata.append(s)

    return returndata


def aliquot_fixed_volume(currentStep, lims, volume, log):
    data = []
    for inp, out in currentStep.input_output_maps:
        if out['output-type'] == 'ResultFile':
            obj = {}
            obj['src_fc'] = inp['uri'].location[0].name.replace(',','_').replace(' ','_')
            obj['src_fc_id'] = inp['uri'].location[0].id
            obj['src_well'] = inp['uri'].location[1]
            obj['dst_fc'] = out['uri'].location[0].name.replace(',','_').replace(' ','_')
            obj['dst_fc_id'] = out['uri'].location[0].id
            obj['dst_well'] = out['uri'].location[1]
            obj['vol'] = volume
            data.append(obj)
    data = sorted(data, key=lambda k: (k['src_fc_id'], k['src_well']))
    return data


def zika_upload_csv(currentStep, lims, wl_filename):
    for out in currentStep.all_outputs():
        if out.name == "Mosquito CSV File":
            for f in out.files:
                lims.request_session.delete(f.uri)
            lims.upload_new_file(out, wl_filename)

def zika_upload_log(currentStep, lims, log_filename):
    for out in currentStep.all_outputs():
        if out.name == "Mosquito Log":
            for f in out.files:
                lims.request_session.delete(f.uri)
            lims.upload_new_file(out, log_filename)

def zika_write_log(log, pid):
    log_filename = "zika_log_" + pid + "_" + date.today().strftime("%y%m%d") + ".log"
    with open(log_filename, "w") as logContext:
        logContext.write("\n".join(log))
    return log_filename

def prepooling(currentStep, lims):
    log = []
    if currentStep.instrument.name == "Zika":
        # Constraints
        zika_min_vol = 0.5  # Possible to run on 0.1
        zika_max_vol = 5
        src_dead_vol = 5
        pool_max_vol = 170

        try:
            # Create dataframe of all transfers incl. transfer volume
            df = zika_calc(currentStep, lims, log, zika_min_vol, src_dead_vol, pool_max_vol)

            # Create worklist file
            wl_filename = zika_wl(df, zika_min_vol, zika_max_vol, src_dead_vol, pool_max_vol, log, currentStep.id)

        except (PoolCollision, PoolOverflow):
            # Upload log and display last log message in LIMS
            zika_upload_log(currentStep, lims, zika_write_log(log, currentStep.id))
            sys.stderr.write(log[-1]+'\n')
            sys.exit(2)
            
        else:
            zika_upload_log(currentStep, lims, zika_write_log(log, currentStep.id))
            zika_upload_csv(currentStep, lims, wl_filename)

            if any("WARNING:" in entry for entry in log):
                sys.stderr.write("CSV-file generated with warnings, please check the Log file\n")
                sys.exit(2)
            else:
                logging.info("Work done")

    else:
        # First thing to do is to grab the volumes of the input artifacts. The method is ... rather unique.
        data = compute_transfer_volume(currentStep, lims, log)
        with open("bravo.csv", "w") as csvContext:
            for s in data:
                if s['vol_to_take'] > MAX_WARNING_VOLUME:
                    log.append("Volume for sample {} is above {}, redo the calculations manually".format(MAX_WARNING_VOLUME, s['name']))
                if s['vol_to_take'] < MIN_WARNING_VOLUME:
                    log.append("Volume for sample {} is below {}, redo the calculations manually".format(MIN_WARNING_VOLUME, s['name']))
                csvContext.write("{0},{1},{2},{3},{4}\n".format(s['src_fc_id'], s['src_well'], s['vol_to_take'], s['dst_fc'], s['dst_well']))
        if log:
            with open("bravo.log", "w") as logContext:
                logContext.write("\n".join(log))

        df = pd.read_csv("bravo.csv", header=None)
        df['dest_row'] = df.apply(lambda row: row[4].split(':')[0], axis=1)
        df['dest_col'] = df.apply(lambda row: int(row[4].split(':')[1]), axis=1)
        df = df.sort_values(['dest_col', 'dest_row']).drop(['dest_row', 'dest_col'], axis=1)
        df.to_csv('bravo.csv', header=False, index=False)

        for out in currentStep.all_outputs():
            # attach the csv file and the log file
            if out.name == "EPP Generated Bravo CSV File":
                attach_file(os.path.join(os.getcwd(), "bravo.csv"), out)
            if log and out.name == "Bravo Log":
                attach_file(os.path.join(os.getcwd(), "bravo.log"), out)
        if log:
            # to get an error display in the lims, you need a non-zero exit code AND a message in STDERR
            sys.stderr.write("Errors were met, please check the Log file\n")
            sys.exit(2)
        else:
            logging.info("Work done")

def zika_wl(df, zika_min_vol, zika_max_vol, src_dead_vol, pool_max_vol, log, pid):
    # Calculate how many buffer wells we need and place them on the destination plate
    if not df[df.name == "buffer"].empty:
        tot_buffer_vol = sum(df[df.name == "buffer"]["transfer_vol"])
        num_buffer_wells = int(tot_buffer_vol // (pool_max_vol - src_dead_vol - zika_max_vol) + 1)
        # Place wells starting from bottom right corner
        all_wells = []
        for l in "ABCDEFGH":
            for n in range(1,13):
                all_wells.append(l+":"+str(n))
        all_wells.reverse()
        buffer_wells = all_wells[0:num_buffer_wells]

    # Determine subtransfers
    wl = pd.DataFrame()
    for idx, row in df.iterrows():
        if row.transfer_vol > zika_max_vol:
            n_splits = int(row.transfer_vol // zika_max_vol + 1)
            split_transfer_vol = row.transfer_vol / n_splits
            row.transfer_vol = split_transfer_vol
            for i in range(0,n_splits):
                wl = wl.append(row)
        else:
            wl = wl.append(row)
    # New index --> subtransfer ID, old index --> transfer ID
    wl.reset_index(inplace = True)
    wl = wl.rename(columns={'index': 'transfer_id'}, inplace = False)

    # Assign buffer src wells, switch if we run out
    iter_buffer_well = iter(buffer_wells)
    current_buffer_well = next(iter_buffer_well)
    current_vol = pool_max_vol
    for idx, row in wl[wl.src_well.isna()].iterrows():
        wl.at[idx,'src_well'] = current_buffer_well
        current_vol -= row.transfer_vol
        if current_vol < src_dead_vol + zika_max_vol:
            next(iter_buffer_well)
            current_vol = pool_max_vol
    # Raise error if buffer well assignment conflicts with pool well assignment
    if any(df.loc[df.id.notna(),"dst_well"].isin(buffer_wells)):
        log.append("ERROR: Assigned pool wells conflict with auto-assigned buffer wells ({}). Buffer wells are assigned starting in the bottom right corner of the destination plate.\n".format(", ".join(buffer_wells)))
        raise PoolCollision()

    # Determine plate layout
    # In a VERY UGLY way TODO
    plate_counts = wl.loc[wl.src_fc_id != wl.dst_fc[0],"src_fc"].value_counts()
    src_plates = pd.DataFrame({"src_fc":plate_counts.index, "count":plate_counts.values})
    src_plates.sort_values(inplace = True, by="count", ascending=False)

    n_src_plates = len(src_plates)
    n_layouts = n_src_plates // 4 + 1

    pos = [2]*n_layouts + [4]*n_layouts + [1]*n_layouts + [5]*n_layouts
    pos = pos[0:n_src_plates]
    layout = list(range(1,n_layouts+1))*(n_src_plates // 2 + 1)
    layout = layout[0:n_src_plates]
    src_plates["src_pos"] = pos
    src_plates["dst_pos"] = 3
    src_plates["layout"] = layout

    # Merge layout info and add buffer transfer info
    wl2 = pd.merge(wl,src_plates,how="left")
    wl2.loc[wl2['src_fc'] == "buffer", 'src_pos'] = 3
    wl2.loc[wl2['src_fc'] == "buffer", 'dst_pos'] = 3

    # Change tips for sample but not for buffer transfers
    wl2['VAR'] = where(wl2['src_fc']=='buffer', '[VAR1]', '[VAR2]')

    # Add row/col info
    wl2["src_row"], wl2["src_col1"] = well2rowcol(wl2.src_well)
    wl2["src_col2"] = wl2.src_col1
    wl2["dst_row"], wl2["dst_col"] = well2rowcol(wl2.dst_well)

    # Transform to integers
    wl2["vol_nl"] = round(wl2.transfer_vol * 1000)
    wl2 = wl2.astype({"src_pos": int, "dst_pos": int, "vol_nl": int}, errors = "ignore")

    # Keep only the worklist-related columns
    wl3 = wl2[["src_pos", "src_col1", "src_col2", "src_row", "dst_pos", "dst_col", "dst_row",
                "vol_nl", "VAR", "layout", "src_fc"]]
    
    # GENERATE WORKLIST
    
    # For buffer transfers, switch tip every n transfers
    switch_every_n = 10
    wl_buffer = wl3[wl3.layout.isna()].sort_values(by = "vol_nl", ascending = False)
    wl_buffer.reset_index(drop = True, inplace = True)
    wl_buffer.loc[switch_every_n::switch_every_n,"VAR"] = "[VAR2]"

    wl_sample = wl3[wl3.layout.notna()].sort_values(by = ["layout","vol_nl"], ascending = [True, False])

    wl_filename = "_".join(["zika_worklist", pid, date.today().strftime("%y%m%d")]) + ".csv"
    with open(wl_filename, "w") as csvContext:
        # Write header
        csvContext.write("worklist,\n")
        csvContext.write("[VAR1]TipChangeStrategy,never,[VAR2]TipChangeStrategy,always\n")
        csvContext.write("COMMENT, This is a Zika advanced worklist for LIMS process {} generated {}\n".format(pid, date.today()))
        csvContext.write("COMMENT, The worklist will enact transfers of {} samples from {} src plate(s) into {} pool(s) via {} layout(s)\n".format(
            len(df[df.id.notna()]), n_src_plates, len(df.dst_well.unique()), n_layouts))
        if not wl_buffer.empty:
            csvContext.write("COMMENT, Please make sure well(s) [{}] of the destination plate are filled with 170 ul buffer\n".format(" ".join(buffer_wells)))

        # Loop over layouts
        for i in range(1, n_layouts + 1):
            # In the first layout, start with the buffer transfers
            if i == 1 and not wl_buffer.empty:
                wl_current = wl_buffer.append(wl_sample[wl_sample.layout == i])
            else:
                wl_current = wl_sample[wl_sample.layout == i]

            # Get the deck layout to print in comment
            sample_deck = src_plates.loc[src_plates.layout == i,["src_fc","src_pos"]]
            deck = pd.merge(pd.DataFrame({"src_pos":[1,2,3,4,5]}), sample_deck, how = "left", on = "src_pos")
            deck.loc[deck.src_pos==3, "src_fc"] = "[Destination plate]"
            deck.fillna("[Empty]", inplace = True)

            csvContext.write("COMMENT, Set up layout {}:    ".format(i) + "     ".join(deck.src_fc))
            csvContext.write("\nPAUSE, 0\n")
            
            # Write transfers
            for idx, row in wl_current.iterrows():
                csvContext.write(",".join(["COPY"] + [str(e) for e in row["src_pos":"VAR"]])+"\n")

    return wl_filename

def zika_calc(currentStep, lims, log, zika_min_vol, src_dead_vol, pool_max_vol):
    # Calculate volumes via zika_vols() for one pooling at a time

    data = make_datastructure(currentStep, lims, log)
    returndata = pd.DataFrame()

    # Get pools and sort by destination row, col
    pools = [art for art in currentStep.all_outputs() if art.type == "Analyte"]
    pools.sort(key=lambda pool: pool.location[1])

    for pool in pools:
        # Replace commas with semicolons, so pool names can be printed in worklist
        pool.name = pool.name.replace(",",";")

        valid_inputs = [x for x in data if x['pool_id'] == pool.id]
        
        target_pool_vol = float(pool.udf["Final Volume (uL)"])
        target_pool_conc = float(pool.udf["Pool Conc. (nM)"])
        
        df = zika_vols(valid_inputs, target_pool_vol, target_pool_conc, pool.name, log,
                        zika_min_vol, src_dead_vol, pool_max_vol)
        returndata = returndata.append(df, ignore_index = True)

    return returndata

class PoolOverflow(Exception):
    pass
class PoolCollision(Exception):
    pass

def zika_vols(samples, target_pool_vol, target_pool_conc, pool_name, log,
              zika_min_vol, src_dead_vol, pool_max_vol):
    # Takes a pooling, then calculates and returns a df w. the associated transfer volumes

    n_src = len(samples)
    target_pool_amount = target_pool_vol * target_pool_conc
    target_sample_amount = target_pool_amount / n_src

    log.append("\nPooling {} samples into {}...".format(n_src,pool_name))
    log.append("Target conc: {} nM, Target vol: {} ul".format(target_pool_conc, target_pool_vol))

    df = pd.DataFrame(samples)

    # Take dead volume into account for calculating transferrable amount
    df = df.rename(columns = {"vol":"full_vol"})
    df["live_vol"] = df.full_vol - src_dead_vol

    # Determine lowest / highest common transfer amount
    df["min_amount"] = zika_min_vol * df.conc
    df["max_amount"] = df.live_vol * df.conc
    highest_min_amount = max(df.min_amount)  # Let highest conc. sample set the ceiling
    lowest_max_amount = min(df.max_amount)  # Let lowest amount sample set the floor

    df["minimized_vol"] = highest_min_amount / df.conc
    pool_min_vol = sum(df.minimized_vol)
    if pool_min_vol > pool_max_vol:
        log.append("ERROR: Overflow in {}. Decrease number of samples or dilute highly concentrated outliers.\n".format(pool_name))
        raise PoolOverflow()

    # Given our input samples, which volumes / concs. are possible as output?
    # Minimize amount
    pool_max_conc = highest_min_amount * n_src / pool_min_vol
    pool_min_conc = highest_min_amount * n_src / pool_max_vol
    # Maximize amount
    pool_min_vol2 = min(pool_min_vol*lowest_max_amount/highest_min_amount, pool_max_vol)
    pool_min_conc2 = pool_max_conc * pool_min_vol2 / pool_max_vol
    # Pack all metrics into a list, to decrease number of input arguments later
    pool_boundaries = [pool_min_vol, pool_min_vol2, pool_max_vol, pool_min_conc, pool_min_conc2, pool_max_conc]

    if highest_min_amount < lowest_max_amount:
        log.append("Pool can be created for conc {}-{} nM and vol {}-{} ul.".format(
            round(pool_min_conc,2), round(pool_max_conc,2), round(pool_min_vol,2), round(pool_max_vol,2)))

        # Nudge conc, if necessary
        if target_pool_conc < pool_min_conc:
            pool_conc = pool_min_conc
        elif target_pool_conc > pool_max_conc:
            pool_conc = pool_max_conc
        else:
            pool_conc = target_pool_conc
        if target_pool_conc != pool_conc:
            log.append("WARNING: Target pool conc {} nM is out of range and adjusted to {} nM".format(
                target_pool_conc, round(pool_conc,2)))
        
        #  Nudge vol, if necessary
        if target_pool_vol < conc2vol(pool_conc, pool_boundaries)[0]:
            pool_vol = conc2vol(pool_conc, pool_boundaries)[0]
        elif target_pool_vol > conc2vol(pool_conc, pool_boundaries)[1]:
            pool_vol = conc2vol(pool_conc, pool_boundaries)[1]
        else:
            pool_vol = target_pool_vol
        if target_pool_vol != pool_vol:
            log.append(
                "WARNING: Target pool vol {} ul is not possible for conc {} nM and is adjusted to {} ul".format(target_pool_vol, round(pool_conc,2), round(pool_vol,2)))
    
        if target_pool_conc == pool_conc and target_pool_vol == pool_vol:
            log.append("Pooling OK")
    else:
        log.append("WARNING: Perfect pooling is not possible, some samples will be below target levels.")
        log.append("Pool conc is maximized to {} nM and pool vol is minimized to {} ul.".format(pool_max_conc,round(pool_min_vol,2)))
        pool_conc = pool_max_conc
        pool_vol = pool_min_vol

    # Append transfer volumes and corresponding fraction of target conc. for each sample
    sample_transfer_amount = pool_conc * pool_vol / n_src
    df["transfer_vol"] = minimum(sample_transfer_amount / df.conc, df.live_vol)
    df["final_target_fraction"] = round((df.transfer_vol * df.conc / pool_vol) / (pool_conc / n_src), 2)

    # If needed, add buffer w/o assigning source
    total_sample_vol = sum(df["transfer_vol"])
    if pool_vol - total_sample_vol > zika_min_vol:
        df = df.append({'name':"buffer",
                        "src_fc":"buffer",
                        "src_fc_id":df["dst_fc"][0],
                        "pool_id":df["pool_id"][0],
                        "dst_fc":df["dst_fc"][0],
                        "dst_well":df["dst_well"][0],
                        "transfer_vol":pool_vol - total_sample_vol},
                        ignore_index = True)
    
    # Report low-conc samples
    low_samples = df[df.final_target_fraction < 1][["name", "final_target_fraction"]]
    if len(low_samples) > 0:
        log.append("The following samples are pooled below target concentration:")
        log.append("Sample\tFraction of target conc.")
        for l in low_samples.values:
            log.append("{}\t{}".format(l[0],l[1]))

    return df

def conc2vol(conc, pool_boundaries):
    # Nudge target vol based on conc. and pool boundaries
    [pool_min_vol, pool_min_vol2, pool_max_vol, pool_min_conc, pool_min_conc2, pool_max_conc] = pool_boundaries
    assert pool_min_conc <= conc <= pool_max_conc

    min_vol = pool_min_vol * pool_max_conc / conc
    max_vol = min(pool_max_vol, pool_min_vol2 * pool_max_conc / conc)
    return (min_vol, max_vol)

def well2rowcol(well_iter):
    # Mosquitos use two integers (row and column) to specify well location
    # In an advanced worklist: startcol, endcol, row
    rows = []
    cols = []
    for well in well_iter:
        [row_letter, col_number] = str.split(well, sep=":")
        rowdict = {}
        for l,n in zip("ABCDEFGH","12345678"):
            rowdict[l] = n
        rows.append(rowdict[row_letter])
        cols.append(col_number)
    return rows, cols

def setup_qpcr(currentStep, lims):
    log = []
    data = aliquot_fixed_volume(currentStep, lims, MIN_WARNING_VOLUME, log)
    with open("bravo.csv", "w") as csvContext:
        for s in data:
            csvContext.write("{0},{1},{2},{3},{4}\n".format(s['src_fc_id'], s['src_well'], s['vol'], s['dst_fc'], s['dst_well']))
    if log:
        with open("bravo.log", "w") as logContext:
            logContext.write("\n".join(log))

    df = pd.read_csv("bravo.csv", header=None)
    df['dest_row'] = df.apply(lambda row: row[4].split(':')[0], axis=1)
    df['dest_col'] = df.apply(lambda row: int(row[4].split(':')[1]), axis=1)
    df = df.sort_values(['dest_col', 'dest_row']).drop(['dest_row', 'dest_col'], axis=1)
    df.to_csv('bravo.csv', header=False, index=False)

    for out in currentStep.all_outputs():
        # attach the csv file and the log file
        if out.name == "EPP Generated Bravo CSV File":
            attach_file(os.path.join(os.getcwd(), "bravo.csv"), out)
        if log and out.name == "Bravo Log":
            attach_file(os.path.join(os.getcwd(), "bravo.log"), out)
    if log:
        # to get an error display in the lims, you need a non-zero exit code AND a message in STDERR
        sys.stderr.write("Errors were met, please check the Log file\n")
        sys.exit(2)
    else:
        logging.info("Work done")


def default_bravo(lims, currentStep, with_total_vol=True):
    checkTheLog = [False]
    dest_plate = []
    with open("bravo.csv", "w") as csvContext:
        with open("bravo.log", "w") as logContext:
            # working directly with the map allows easier input/output handling
            for art_tuple in currentStep.input_output_maps:
            # filter out result files
                if art_tuple[0]['uri'].type == 'Analyte' and art_tuple[1]['uri'].type == 'Analyte':
                    source_fc = art_tuple[0]['uri'].location[0].name
                    source_well = art_tuple[0]['uri'].location[1]
                    dest_fc = art_tuple[1]['uri'].location[0].id
                    dest_well = art_tuple[1]['uri'].location[1]
                    dest_fc_name = art_tuple[1]['uri'].location[0].name
                    dest_plate.append(dest_fc_name)
                    if with_total_vol:
                        try:
                            # might not be filled in
                            final_volume = art_tuple[1]['uri'].udf["Total Volume (uL)"]
                        except KeyError:
                            logContext.write("No Total Volume found for sample {0}\n".format(art_tuple[0]['uri'].samples[0].name))
                            checkTheLog[0] = True
                        else:
                            volume = calc_vol(art_tuple, logContext, checkTheLog)
                            csvContext.write("{0},{1},{2},{3},{4},{5}\n".format(source_fc, source_well, volume, dest_fc, dest_well, final_volume))
                    else:
                        volume = calc_vol(art_tuple, logContext, checkTheLog)
                        csvContext.write("{0},{1},{2},{3},{4}\n".format(source_fc, source_well, volume, dest_fc, dest_well))

    df = pd.read_csv("bravo.csv", header=None)
    df['dest_row'] = df.apply(lambda row: row[4].split(':')[0], axis=1)
    df['dest_col'] = df.apply(lambda row: int(row[4].split(':')[1]), axis=1)
    df = df.sort_values(['dest_col', 'dest_row']).drop(['dest_row', 'dest_col'], axis=1)
    df.to_csv('bravo.csv', header=False, index=False)
    
    # For now only one output plate is supported:
    if len(list(set(dest_plate))) == 1:
        dest_plate_name = list(set(dest_plate))[0]
        os.rename("bravo.csv", "{}_bravo.csv".format(dest_plate_name))
        os.rename("bravo.log", "{}_bravo.log".format(dest_plate_name))
    else:
        sys.stderr.write("ERROR: Multiple output plates!\n")
        sys.exit(2)

    for out in currentStep.all_outputs():
        # attach the csv file and the log file
        if out.name == "EPP Generated Bravo CSV File":
            for f in out.files:
                lims.request_session.delete(f.uri)
            lims.upload_new_file(out, "{}_bravo.csv".format(dest_plate_name))
        if out.name == "Bravo Log":
            for f in out.files:
                lims.request_session.delete(f.uri)
            lims.upload_new_file(out, "{}_bravo.log".format(dest_plate_name))
    if checkTheLog[0]:
        # to get an eror display in the lims, you need a non-zero exit code AND a message in STDERR
        sys.stderr.write("Errors were met, please check the Log file\n")
        sys.exit(2)
    else:
        logging.info("Work done")


def dilution(currentStep):
    checkTheLog = [False]
    # Read in the presets of minimum required conc for setup workset, maximum conc for dilution and minimum volume for dilution
    if "SMARTer Pico RNA" in currentStep.input_output_maps[0][0]['uri'].workflow_stages[0].workflow.name:
        preset = Dilution_preset['Smarter pico']
    # Use the values set in the step UDFs; if not set, use the default values
    min_required_conc = currentStep.udf['Minimum required conc for workset (ng/ul)'] if currentStep.udf['Minimum required conc for workset (ng/ul)'] else preset[0]
    max_conc_for_dilution = currentStep.udf['Maximum conc for dilution (ng/ul)'] if currentStep.udf['Maximum conc for dilution (ng/ul)'] else preset[1]
    min_vol_for_dilution = currentStep.udf['Minimum volume for dilution (ul)'] if currentStep.udf['Minimum volume for dilution (ul)'] else preset[2]
    with open("bravo.csv", "w") as csvContext:
        with open("bravo.log", "w") as logContext:
            # working directly with the map allows easier input/output handling
            for art_tuple in currentStep.input_output_maps:
            # filter out result files
                if art_tuple[0]['uri'].type == 'Analyte' and art_tuple[1]['uri'].type == 'Analyte':
                    source_fc = art_tuple[0]['uri'].location[0].name
                    source_well = art_tuple[0]['uri'].location[1]
                    dest_fc = art_tuple[1]['uri'].location[0].id
                    dest_well = art_tuple[1]['uri'].location[1]
                    try:
                        # Only ng/ul or ng/uL are supported
                        assert art_tuple[0]['uri'].udf['Conc. Units'] in ["ng/ul", "ng/uL"]
                        # Fill in all necessary UDFs
                        art_tuple[1]['uri'].udf['Concentration'] = art_tuple[0]['uri'].udf['Concentration']
                        art_tuple[1]['uri'].udf['Conc. Units'] = art_tuple[0]['uri'].udf['Conc. Units']
                        # Case that sample concentration lower than the minimum required conc for setup workset
                        if art_tuple[1]['uri'].udf['Concentration'] < min_required_conc:
                            if art_tuple[0]['uri'].udf['Volume (ul)'] >= min_vol_for_dilution:
                                art_tuple[1]['uri'].udf['Volume to take (uL)'] = min_vol_for_dilution
                                art_tuple[1]['uri'].udf['Final Concentration'] = art_tuple[1]['uri'].udf['Concentration']
                                art_tuple[1]['uri'].udf['Final Volume (uL)'] = min_vol_for_dilution
                                logContext.write("WARN : Sample {0} located {1} {2} has a LOWER conc than {3}. Take {4} ul directly into the dilution plate.\n".format(art_tuple[1]['uri'].samples[0].name,art_tuple[0]['uri'].location[0].name, art_tuple[0]['uri'].location[1],min_required_conc,min_vol_for_dilution))
                            else:
                                art_tuple[1]['uri'].udf['Volume to take (uL)'] = 0
                                art_tuple[1]['uri'].udf['Final Concentration'] = 0
                                art_tuple[1]['uri'].udf['Final Volume (uL)'] = 0
                                logContext.write("ERROR : Sample {0} located {1} {2} has a LOWER conc than {3} and total volume less than {4} ul. It is skipped in dilution.\n".format(art_tuple[1]['uri'].samples[0].name,art_tuple[0]['uri'].location[0].name, art_tuple[0]['uri'].location[1],min_required_conc,min_vol_for_dilution))
                        # Case that sample concentration higher than the maximum conc for dilution
                        elif art_tuple[1]['uri'].udf['Concentration'] > max_conc_for_dilution:
                            art_tuple[1]['uri'].udf['Volume to take (uL)'] = 0
                            art_tuple[1]['uri'].udf['Final Concentration'] = 0
                            art_tuple[1]['uri'].udf['Final Volume (uL)'] = 0
                            logContext.write("ERROR : Sample {0} located {1} {2} has a HIGHER conc than {3}. It is skipped in dilution.\n".format(art_tuple[1]['uri'].samples[0].name,art_tuple[0]['uri'].location[0].name, art_tuple[0]['uri'].location[1],max_conc_for_dilution))
                        # Case that dilution will be done with 2uL sample
                        elif art_tuple[1]['uri'].udf['Concentration'] <= max_conc_for_dilution and art_tuple[1]['uri'].udf['Concentration'] > float(min_required_conc*min_vol_for_dilution)/MIN_WARNING_VOLUME:
                            if art_tuple[0]['uri'].udf['Volume (ul)'] >= MIN_WARNING_VOLUME:
                                final_conc = min_required_conc
                                step = 0.25
                                while final_conc <= max_conc_for_dilution*MIN_WARNING_VOLUME/MAX_WARNING_VOLUME:
                                    if float(art_tuple[1]['uri'].udf['Concentration']*MIN_WARNING_VOLUME/final_conc) <= MAX_WARNING_VOLUME:
                                        art_tuple[1]['uri'].udf['Volume to take (uL)'] = MIN_WARNING_VOLUME
                                        art_tuple[1]['uri'].udf['Final Concentration'] = final_conc
                                        art_tuple[1]['uri'].udf['Final Volume (uL)'] = float(art_tuple[1]['uri'].udf['Concentration']*MIN_WARNING_VOLUME/final_conc)
                                        logContext.write("INFO : Sample {0} looks okay.\n".format(art_tuple[1]['uri'].samples[0].name))
                                        break
                                    else:
                                        final_conc = final_conc+0.25
                            else:
                                art_tuple[1]['uri'].udf['Volume to take (uL)'] = 0
                                art_tuple[1]['uri'].udf['Final Concentration'] = 0
                                art_tuple[1]['uri'].udf['Final Volume (uL)'] = 0
                                logContext.write("ERROR : Sample {0} located {1} {2} has a LOWER volume than {3} ul. It is skipped in dilution.\n".format(art_tuple[1]['uri'].samples[0].name,art_tuple[0]['uri'].location[0].name, art_tuple[0]['uri'].location[1],MIN_WARNING_VOLUME))
                        # Case that more than 2uL sample is needed for dilution
                        elif art_tuple[1]['uri'].udf['Concentration'] <= float(min_required_conc*min_vol_for_dilution)/MIN_WARNING_VOLUME and art_tuple[1]['uri'].udf['Concentration'] >= min_required_conc:
                            if art_tuple[0]['uri'].udf['Volume (ul)'] >= float(min_required_conc*min_vol_for_dilution/art_tuple[1]['uri'].udf['Concentration']):
                                art_tuple[1]['uri'].udf['Volume to take (uL)'] = float(min_required_conc*min_vol_for_dilution/art_tuple[1]['uri'].udf['Concentration'])
                                art_tuple[1]['uri'].udf['Final Concentration'] = min_required_conc
                                art_tuple[1]['uri'].udf['Final Volume (uL)'] = min_vol_for_dilution
                                logContext.write("INFO : Sample {0} looks okay.\n".format(art_tuple[1]['uri'].samples[0].name))
                            else:
                                art_tuple[1]['uri'].udf['Volume to take (uL)'] = 0
                                art_tuple[1]['uri'].udf['Final Concentration'] = 0
                                art_tuple[1]['uri'].udf['Final Volume (uL)'] = 0
                                logContext.write("ERROR : Sample {0} located {1} {2} has a LOWER volume than {3} ul. It is skipped in dilution.\n".format(art_tuple[1]['uri'].samples[0].name,art_tuple[0]['uri'].location[0].name, art_tuple[0]['uri'].location[1],float(min_required_conc*min_vol_for_dilution/art_tuple[1]['uri'].udf['Concentration'])))
                    except KeyError as e:
                        logContext.write("ERROR : The input artifact is lacking a field : {0}\n".format(e))
                        checkTheLog[0] = True
                    except AssertionError:
                        logContext.write("ERROR : This script expects the concentration to be in ng/ul or ng/uL, this does not seem to be the case.\n")
                        checkTheLog[0] = True
                    except ZeroDivisionError:
                        logContext.write("ERROR: Sample {0} has a concentration of 0\n".format(art_tuple[1]['uri'].samples[0].name))
                        checkTheLog[0] = True

                    art_tuple[1]['uri'].put()
                    csvContext.write("{0},{1},{2},{3},{4},{5}\n".format(source_fc, source_well, art_tuple[1]['uri'].udf['Volume to take (uL)'], dest_fc, dest_well, art_tuple[1]['uri'].udf['Final Volume (uL)']))

    df = pd.read_csv("bravo.csv", header=None)
    df['dest_row'] = df.apply(lambda row: row[4].split(':')[0], axis=1)
    df['dest_col'] = df.apply(lambda row: int(row[4].split(':')[1]), axis=1)
    df = df.sort_values(['dest_col', 'dest_row']).drop(['dest_row', 'dest_col'], axis=1)
    df.to_csv('bravo.csv', header=False, index=False)

    for out in currentStep.all_outputs():
        # attach the csv file and the log file
        if out.name == "EPP Generated Bravo CSV File":
            attach_file(os.path.join(os.getcwd(), "bravo.csv"), out)
        if out.name == "Bravo Log":
            attach_file(os.path.join(os.getcwd(), "bravo.log"), out)
    if checkTheLog[0]:
        # to get an eror display in the lims, you need a non-zero exit code AND a message in STDERR
        sys.stderr.write("Errors were met, please check the Log file\n")
        sys.exit(2)
    else:
        logging.info("Work done")


def normalization(current_step):
    log = []
    with open("bravo.csv", "w") as csv:
        for art in current_step.input_output_maps:
            src = art[0]["uri"]
            dest = art[1]["uri"]
            if src.type == dest.type == "Analyte":
                # Source sample:
                src_plate = src.location[0].id
                src_well = src.location[1]
                try:
                    src_tot_volume = float(src.udf["Volume (ul)"])
                except:
                    src_tot_volume = 999999
                    log.append("WARNING: No volume found for input sample {0}".format(src.samples[0].name))
                try:
                    src_volume = float(dest.udf["Volume to take (uL)"])
                except:
                    sys.stderr.write("Field 'Volume to take (uL)' is empty for artifact {0}\n".format(dest.name))
                    sys.exit(2)
                if "Concentration" in src.udf:
                    src_conc = src.udf["Concentration"]
                    if src.udf["Conc. Units"] != "nM":
                        log.append("ERROR: No valid concentration found for sample {0}".format(src.samples[0].name))
                elif "Normalized conc. (nM)" in src.udf:
                    src_conc = src.udf["Normalized conc. (nM)"]
                else:
                    sys.stderr.write("Non input concentration found for sample {0}\n".format(dest.name))
                    sys.exit(2)


                # Diluted sample:
                dest_plate = dest.location[0].id
                dest_well = dest.location[1]
                try:
                    dest_conc = dest.udf["Normalized conc. (nM)"]
                except:
                    sys.stderr.write("Field 'Normalized conc. (nM)' is empty for artifact {0}\n".format(dest.name))
                    sys.exit(2)
                if src_conc < dest_conc:
                    log.append("ERROR: Too low concentration for sample {0}".format(src.samples[0].name))
                else:
                    # Warn if volume to take > volume available or max volume is
                    # exceeded but still do the calculation:
                    if src_volume > src_tot_volume:
                        log.append("WARNING: Not enough available volume of sample {0}".format(src.samples[0].name))
                    final_volume = src_conc * src_volume / dest_conc
                    if final_volume > MAX_WARNING_VOLUME:
                        log.append("WARNING: Maximum volume exceeded for sample {0}".format(src.samples[0].name))
                    csv.write("{0},{1},{2},{3},{4},{5}\n".format(src_plate, src_well, src_volume, dest_plate, dest_well, final_volume))
    if log:
        with open("bravo.log", "w") as log_context:
            log_context.write("\n".join(log))

    df = pd.read_csv("bravo.csv", header=None)
    df['dest_row'] = df.apply(lambda row: row[4].split(':')[0], axis=1)
    df['dest_col'] = df.apply(lambda row: int(row[4].split(':')[1]), axis=1)
    df = df.sort_values(['dest_col', 'dest_row']).drop(['dest_row', 'dest_col'], axis=1)
    df.to_csv('bravo.csv', header=False, index=False)

    for out in current_step.all_outputs():
        # attach the csv file and the log file
        if out.name == "EPP Generated Bravo CSV File for Normalization":
            attach_file(os.path.join(os.getcwd(), "bravo.csv"), out)
        elif out.name == "Bravo Log" and log:
            attach_file(os.path.join(os.getcwd(), "bravo.log"), out)
    if log:
        # to get an eror display in the lims, you need a non-zero exit code AND a message in STDERR
        sys.stderr.write("Errors were met, please check the log file\n")
        sys.exit(2)
    else:
        logging.info("Work done")


def sample_dilution_before_QC(currentStep):
    checkTheLog = [False]
    mode = currentStep.udf['Mode']
    with open("bravo.csv", "w") as csvContext:
        with open("bravo.log", "w") as logContext:
            for art_tuple in currentStep.input_output_maps:
                if art_tuple[1]['output-generation-type'] == 'PerInput':
                    source_fc = art_tuple[0]['uri'].location[0].name
                    source_well = art_tuple[0]['uri'].location[1]
                    dest_fc = art_tuple[1]['uri'].location[0].id
                    dest_well = art_tuple[1]['uri'].location[1]
                    submitted_sam = art_tuple[0]['uri'].samples[0]
                    sample_name = submitted_sam.name
                    # Retrieve the previous aggregate values for concentration and volume. Note that aggregated values have a higher priority than customer values
                    try:
                        aggregate_conc = art_tuple[0]['uri'].udf['Concentration']
                        aggregate_vol = art_tuple[0]['uri'].udf['Volume (ul)']
                        input_conc = aggregate_conc
                        input_vol = aggregate_vol
                    except KeyError:
                        logContext.write("WARNING : Sample {0} does not have aggregated values for concentration or volume. Trying with customer values instead.\n".format(sample_name))
                        # Retrieve customer values for concentration and volume
                        try:
                            customer_conc = submitted_sam.udf['Customer Conc']
                            customer_vol = submitted_sam.udf['Customer Volume']
                            input_conc = customer_conc
                            input_vol = customer_vol
                        except KeyError:
                            logContext.write("ERROR : Sample {0} does not have customer values for concentration or volume. It will be skipped.\n".format(sample_name))
                            checkTheLog[0] = True
                            continue

                    # Volume for the dilution mode
                    if mode == 'Dilution to a new plate':
                        # Error when the input volume is lower than the minimum pipetting volume
                        if input_vol < MIN_WARNING_VOLUME:
                            logContext.write("ERROR : Sample {0} has too little volume for dilution.\n".format(sample_name))
                            checkTheLog[0] = True
                            continue
                        # Fetch the set value of volume to take. Otherwise take as little as possible
                        else:
                            try:
                                vol_taken = art_tuple[1]['uri'].udf['Volume to take (uL)']
                                if vol_taken > input_vol:
                                    logContext.write("ERROR : Sample {0} has a volume {1} uL which is not enough for taking {2} uL.\n".format(sample_name, customer_vol, vol_taken))
                                    checkTheLog[0] = True
                                    continue
                            except KeyError:
                                vol_taken = MIN_WARNING_VOLUME
                    # Volume for the aliquotation mode
                    elif mode == 'Add EB to original plate':
                        vol_taken = input_vol

                    # 1st priority: Aim concentration
                    try:
                        final_conc = art_tuple[1]['uri'].udf['Final Concentration']
                        final_vol = input_conc*vol_taken/final_conc
                        dilution_fold = final_vol/vol_taken
                        EB_vol = final_vol-vol_taken
                    except KeyError:
                        # 2nd priority: Final volume
                        try:
                            final_vol = art_tuple[1]['uri'].udf['Final Volume (uL)']
                            final_conc = input_conc*vol_taken/final_vol
                            dilution_fold = final_vol/vol_taken
                            EB_vol = final_vol-vol_taken
                        except KeyError:
                            # 3rd priority: Dilution fold
                            try:
                                dilution_fold = art_tuple[1]['uri'].udf['Dilution Fold']
                                final_vol = vol_taken*dilution_fold
                                final_conc = input_conc/dilution_fold
                                EB_vol = final_vol-vol_taken
                            except KeyError:
                            # Error when no value is set
                                logContext.write("ERROR : Sample {0} does not have a preset value.\n".format(sample_name))
                                checkTheLog[0] = True
                                continue
                    # Whether final volume is higher than the capacity of plate
                    if final_vol <= MAX_WARNING_VOLUME and final_conc <= input_conc:
                        art_tuple[1]['uri'].udf['Final Concentration'] = final_conc
                        art_tuple[1]['uri'].udf['Final Volume (uL)'] = final_vol
                        art_tuple[1]['uri'].udf['Dilution Fold'] = dilution_fold
                        art_tuple[1]['uri'].udf['Volume to take (uL)'] = vol_taken
                        art_tuple[1]['uri'].put()
                        if mode == 'Dilution to a new plate':
                            csvContext.write("{0},{1},{2},{3},{4},{5}\n".format(source_fc, source_well, vol_taken, dest_fc, dest_well, final_vol))
                        elif mode == 'Add EB to original plate':
                            csvContext.write("{0},{1},{2},{3},{4}\n".format('EB_plate', 'A1', EB_vol, source_fc, source_well))
                    elif final_vol > MAX_WARNING_VOLUME:
                        logContext.write("ERROR : Sample {0} will have a dilution higher than max allowed volume {1}.\n".format(sample_name, MAX_WARNING_VOLUME))
                        checkTheLog[0] = True
                        continue
                    elif final_conc > input_conc:
                        logContext.write("ERROR : Sample {0} will have a final concentration higher than the input concentration {1}.\n".format(sample_name, input_conc))
                        checkTheLog[0] = True
                        continue

    df = pd.read_csv("bravo.csv", header=None)
    df['dest_row'] = df.apply(lambda row: row[4].split(':')[0], axis=1)
    df['dest_col'] = df.apply(lambda row: int(row[4].split(':')[1]), axis=1)
    df = df.sort_values(['dest_col', 'dest_row']).drop(['dest_row', 'dest_col'], axis=1)
    df.to_csv('bravo.csv', header=False, index=False)

    for out in currentStep.all_outputs():
        # attach the csv file and the log file
        if out.name == "EPP Generated Bravo CSV File":
            attach_file(os.path.join(os.getcwd(), "bravo.csv"), out)
        if out.name == "Bravo Log":
            attach_file(os.path.join(os.getcwd(), "bravo.log"), out)
    if checkTheLog[0]:
        # to get an eror display in the lims, you need a non-zero exit code AND a message in STDERR
        sys.stderr.write("Errors were met, please check the Log file\n")
        sys.exit(2)
    else:
        logging.info("Work done")


def main(lims, args):
    currentStep = Process(lims, id=args.pid)
    if currentStep.type.name in ['Library Pooling (HiSeq X) 1.0']:
        check_barcode_collision(currentStep)
        prepooling(currentStep, lims)
    elif currentStep.type.name in ['Pre-Pooling (MiSeq) 4.0', 'Pre-Pooling (Illumina SBS) 4.0', 'Library Pooling (RAD-seq) v1.0', 'Library Pooling (TruSeq Small RNA) 1.0', 'Pre-Pooling (NovaSeq) v2.0', 'Pre-Pooling (NextSeq) v1.0']:
        prepooling(currentStep, lims)
    elif currentStep.type.name in ['Library Normalization (HiSeq X) 1.0', 'Library Normalization (Illumina SBS) 4.0', 'Library Normalization (MiSeq) 4.0', 'Library Normalization (NovaSeq) v2.0', 'Library Normalization (NextSeq) v1.0']:
        normalization(currentStep)
    elif currentStep.type.name == 'Library Pooling (RAD-seq) 1.0':
        default_bravo(lims, currentStep, False)
    elif currentStep.type.name == 'Diluting Samples':
        dilution(currentStep)
    elif currentStep.type.name == 'Sample Dilution Before QC':
        sample_dilution_before_QC(currentStep)
    elif currentStep.type.name in ['qPCR QC (Library Validation) 4.0', 'qPCR QC (Dilution Validation) 4.0']:
        setup_qpcr(currentStep, lims)
    else:
        default_bravo(lims, currentStep)


def calc_vol(art_tuple, logContext, checkTheLog):
    try:
        # not handling different units yet. Might be needed at some point.
        assert art_tuple[0]['uri'].udf['Conc. Units'] in ["ng/ul", "ng/uL"]
        amount_ng = art_tuple[1]['uri'].udf['Amount taken (ng)']
        try:
            if art_tuple[0]['uri'].parent_process.type.name == "Diluting Samples":
                conc = art_tuple[0]['uri'].udf['Final Concentration']
                org_vol = art_tuple[0]['uri'].udf['Final Volume (uL)']
            else:
                conc = art_tuple[0]['uri'].udf['Concentration']
                org_vol = art_tuple[0]['uri'].udf['Volume (ul)']
        except AttributeError:
            conc = art_tuple[0]['uri'].udf['Concentration']
            org_vol = art_tuple[0]['uri'].udf['Volume (ul)']
        volume = float(amount_ng) / float(conc)

        if org_vol < MIN_WARNING_VOLUME:
            logContext.write("WARN : Sample {0} located {1} {2}  has a LOW original volume : {3}\n".format(art_tuple[1]['uri'].samples[0].name,
                                                                                                  art_tuple[0]['uri'].location[0].name, art_tuple[0]['uri'].location[1], org_vol))
            volume = min(org_vol, volume)
            checkTheLog[0] = True
        elif volume < MIN_WARNING_VOLUME:
            # arbitrarily determined by Sverker Lundin
            logContext.write("WARN : Sample {0} located {1} {2}  has a LOW pippetting volume: {3}\n".format(art_tuple[1]['uri'].samples[0].name,
                                                                                                  art_tuple[0]['uri'].location[0].name, art_tuple[0]['uri'].location[1], volume))
            checkTheLog[0] = True
        elif volume > org_vol or volume > art_tuple[1]['uri'].udf['Total Volume (uL)']:
            # check against the "original sample volume" and the "total dilution volume"
            new_volume = min(org_vol, art_tuple[1]['uri'].udf['Total Volume (uL)'])
            if org_vol <= art_tuple[1]['uri'].udf['Total Volume (uL)']:
                logContext.write("WARN : Sample {0} located {1} {2}  has a HIGHER volume than the original: {3}, over {4}. Take original volume: {4}\n".format(art_tuple[1]['uri'].samples[0].name,
                                                                                                             art_tuple[0]['uri'].location[0].name, art_tuple[0]['uri'].location[1], volume, org_vol))
            else:
                logContext.write("WARN : Sample {0} located {1} {2}  has a HIGHER volume than the total: {3}, over {4}. Take total volume: {4}\n".format(art_tuple[1]['uri'].samples[0].name,
                                                                                                                             art_tuple[0]['uri'].location[0].name, art_tuple[0]['uri'].location[1], volume, art_tuple[1]['uri'].udf["Total Volume (uL)"]))
            volume = new_volume
            checkTheLog[0] = False
        else:
            logContext.write("INFO : Sample {0} looks okay.\n".format(art_tuple[1]['uri'].samples[0].name))
        return "{0:.2f}".format(volume)
    except KeyError as e:
        logContext.write("ERROR : The input artifact is lacking a field : {0}\n".format(e))
        checkTheLog[0] = True
    except AssertionError:
        logContext.write("ERROR : This script expects the concentration to be in ng/ul or ng/uL, this does not seem to be the case.\n")
        checkTheLog[0] = True
    except ZeroDivisionError:
        logContext.write("ERROR: Sample {0} has a concentration of 0\n".format(art_tuple[1]['uri'].samples[0].name))
        checkTheLog[0] = True
    # this allows to still write the file. Won't be readable though
    return "#ERROR#"

def check_barcode_collision(step):
    for output in step.all_outputs():
        barcodes=[]
        if output.type == "Analyte":
            for io in step.input_output_maps:
                if io[1]['limsid'] == output.id:
                    barcode=find_barcode(io[0]['uri'])
                    if barcode not in barcodes:
                        barcodes.append(find_barcode(io[0]['uri']))
                    else:
                        raise Exception("Similar barcodes {0} in pool {}".format(barcode, output.id))

def find_barcode(artifact):
        if len(artifact.samples) == 1 and artifact.reagent_labels:
            reagent_label_name=artifact.reagent_labels[0].upper()
            idxs = TENX_PAT.findall(reagent_label_name)
            if idxs:
                # Put in tuple with empty string as second index to
                # match expected type:
                idxs = (idxs[0], "")
            else:
                try:
                    idxs = IDX_PAT.findall(reagent_label_name)[0]
                except IndexError:
                    try:
                        # we only have the reagent label name.
                        rt = lims.get_reagent_types(name=reagent_label_name)[0]
                        idxs = IDX_PAT.findall(rt.sequence)[0]
                    except:
                        return ("NoIndex","")

            return idxs
        else:
            if artifact == artifact.samples[0].artifact:
                return None
            else:
                next_artifact=None
                for iomap in artifact.parent_process.input_output_maps:
                    if iomap[1]['uri'].id == artifact.id:
                        next_artifact=iomap[0]['uri']
                return find_barcode(next_artifact)


if __name__ == "__main__":
    parser = ArgumentParser(description=DESC)
    parser.add_argument('--pid',
                        help='Lims id for current Process')
    args = parser.parse_args()

    lims = Lims(BASEURI, USERNAME, PASSWORD)
    lims.check_version()
    main(lims, args)
